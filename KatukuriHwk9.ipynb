{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://comet-28-71.sdsc.edu:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2b2bebc143c8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://comet-28-71.sdsc.edu:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"com.databricks.spark.csv\").option(\"header\",\"false\").option(\"inferSchema\", \"true\").load(\"oasis/netflow_day-11\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[_c0: int, _c1: int, _c2: string, _c3: string, _c4: int, _c5: string, _c6: string, _c7: int, _c8: int, _c9: bigint, _c10: bigint]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.printSchema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.selectExpr(\"_c0 as Time\",\"_c1 as Duration\",\"_c2 as SrcDevice\",\"_c3 as DstDevice\",\"_c4 as Protocol\",\"_c5 as SrcPort\",\"_c6 as DstPort\",\"_c7 as SrcPackets\",\"_c8 as DstPackets\",\"_c9 as SrcBytes\",\"_c10 as DstBytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[Time: int, Duration: int, SrcDevice: string, DstDevice: string, Protocol: int, SrcPort: string, DstPort: string, SrcPackets: int, DstPackets: int, SrcBytes: bigint, DstBytes: bigint]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.withColumn(\"Duration\", df1[\"Duration\"].cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.withColumn(\"SrcPackets\",df2[\"SrcPackets\"].cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.withColumn(\"DstPackets\",df2[\"DstPackets\"].cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.withColumn(\"SrcBytes\",df2[\"SrcBytes\"].cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.withColumn(\"DstBytes\",df2[\"DstBytes\"].cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[Time: int, Duration: double, SrcDevice: string, DstDevice: string, Protocol: int, SrcPort: string, DstPort: string, SrcPackets: double, DstPackets: double, SrcBytes: double, DstBytes: double]>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.select(\"Duration\",\"SrcPackets\",\"DstPackets\",\"SrcBytes\",\"DstBytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[Duration: double, SrcPackets: double, DstPackets: double, SrcBytes: double, DstBytes: double]>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------+--------+--------+\n",
      "|Duration|SrcPackets|DstPackets|SrcBytes|DstBytes|\n",
      "+--------+----------+----------+--------+--------+\n",
      "|     0.0|       1.0|       0.0|    64.0|     0.0|\n",
      "|     0.0|       1.0|       0.0|    75.0|     0.0|\n",
      "|     0.0|       1.0|       0.0|    69.0|     0.0|\n",
      "|     0.0|       5.0|       0.0|   400.0|     0.0|\n",
      "|     0.0|       1.0|       0.0|    58.0|     0.0|\n",
      "+--------+----------+----------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"Duration\",\"SrcPackets\",\"DstPackets\",\"SrcBytes\",\"DstBytes\"],\n",
    "    outputCol=\"feature Vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = assembler.transform(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------+--------+--------+--------------------+\n",
      "|Duration|SrcPackets|DstPackets|SrcBytes|DstBytes|      feature Vector|\n",
      "+--------+----------+----------+--------+--------+--------------------+\n",
      "|     0.0|       1.0|       0.0|    64.0|     0.0|(5,[1,3],[1.0,64.0])|\n",
      "|     0.0|       1.0|       0.0|    75.0|     0.0|(5,[1,3],[1.0,75.0])|\n",
      "|     0.0|       1.0|       0.0|    69.0|     0.0|(5,[1,3],[1.0,69.0])|\n",
      "|     0.0|       5.0|       0.0|   400.0|     0.0|(5,[1,3],[5.0,400...|\n",
      "|     0.0|       1.0|       0.0|    58.0|     0.0|(5,[1,3],[1.0,58.0])|\n",
      "+--------+----------+----------+--------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2962.18356814    169.5142439     164.6317104   46816.85182762\n",
      " 127952.78854591]\n",
      "[3.05003077e+05 1.10199183e+09 3.96508103e+08 2.04425469e+11\n",
      " 5.12937822e+10]\n",
      "[2.83887000e+05 5.62064529e+07 1.13553349e+08 2.70663682e+09\n",
      " 1.52686383e+11]\n",
      "[1.25190476e+05 1.23688736e+07 3.18227680e+07 8.58889649e+08\n",
      " 2.58732086e+10]\n",
      "[2.79597705e+05 2.28936993e+08 9.97122548e+07 3.49151919e+10\n",
      " 1.39353598e+10]\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(k=5,featuresCol=\"feature Vector\",predictionCol=\"cluster\")\n",
    "model = kmeans.fit(output)\n",
    "centers = model.clusterCenters()\n",
    "for x in centers:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import BisectingKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2961.70742148   131.7293426    102.06339279 39807.05962848\n",
      " 86766.22687638]\n",
      "[6.87764776e+04 6.85910329e+06 7.90474261e+06 1.17666230e+09\n",
      " 5.21702711e+09]\n",
      "[1.39294048e+05 3.21155469e+07 3.67006748e+07 5.01199114e+09\n",
      " 2.14358246e+10]\n",
      "[2.44530316e+05 1.06348163e+09 4.32713369e+08 1.66971306e+11\n",
      " 4.86456341e+10]\n",
      "[2.53803404e+05 5.03489389e+07 1.01633320e+08 2.41869228e+09\n",
      " 1.36434739e+11]\n"
     ]
    }
   ],
   "source": [
    "bkm = BisectingKMeans(k=5,featuresCol=\"feature Vector\",predictionCol=\"cluster\")\n",
    "model = bkm.fit(output)\n",
    "centers = model.clusterCenters()\n",
    "for x in centers:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "json = spark.read.json(\"oasis/wls_day-11\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[AuthenticationPackage: string, Destination: string, DomainName: string, EventID: bigint, FailureReason: string, LogHost: string, LogonID: string, LogonType: bigint, LogonTypeDescription: string, ParentProcessID: string, ParentProcessName: string, ProcessID: string, ProcessName: string, ServiceName: string, Source: string, Status: string, SubjectDomainName: string, SubjectLogonID: string, SubjectUserName: string, Time: bigint, UserName: string]>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83183265"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.createGlobalTempView(\"event\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|EventID|number|\n",
      "+-------+------+\n",
      "|   4625|356966|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select EventID,count(*) as number from global_temp.event where EventID = 4625 group by EventID\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|EventID|number|\n",
      "+-------+------+\n",
      "|   4609|    21|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select EventID, count(*) as number from global_temp.event where EventID = 4609 group by EventID\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
